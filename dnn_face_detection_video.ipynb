{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"dnn_face_detection_video.ipynb","provenance":[],"authorship_tag":"ABX9TyPKIUrzUvikv1CVIpICm8oO"},"kernelspec":{"name":"python3","display_name":"Python 3"},"accelerator":"GPU"},"cells":[{"cell_type":"code","metadata":{"id":"HghQ2X9QXBLe","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1634572026593,"user_tz":-540,"elapsed":17713,"user":{"displayName":"송정현","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"03834848729540117659"}},"outputId":"8ac4d93e-68a8-4a09-995c-41750342be70"},"source":["# 내 구글 드라이브에 연동\n","from google.colab import drive\n","drive.mount('/content/gdrive')"],"execution_count":1,"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/gdrive\n"]}]},{"cell_type":"code","metadata":{"id":"gQCcECj4X8xA","executionInfo":{"status":"ok","timestamp":1634572029553,"user_tz":-540,"elapsed":851,"user":{"displayName":"송정현","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"03834848729540117659"}}},"source":["# 필요한 패키지와 모듈을 불러옴\n","import cv2\n","import numpy as np\n","import time\n","import io\n","import base64\n","from IPython.display import HTML"],"execution_count":2,"outputs":[]},{"cell_type":"code","metadata":{"id":"NIc7qbYzYG56","colab":{"base_uri":"https://localhost:8080/","height":441,"output_embedded_package_id":"1p7K3jdkNgA9KuuMAXFsF-FI2nVvitlwt"},"executionInfo":{"status":"ok","timestamp":1634572036744,"user_tz":-540,"elapsed":5463,"user":{"displayName":"송정현","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"03834848729540117659"}},"outputId":"120c2584-12e8-4c6f-f562-e5c9e8a91770"},"source":["# Detection 하기 전에 원본 동영상을 Display\n","video = io.open('/content/gdrive/MyDrive/DNN_Face_Detection/video/kim.mp4', 'r+b').read()\n","encoded = base64.b64encode(video)\n","HTML(data='''<video width=\"50%\" controls>\n","                <source src=\"data:video/mp4;base64,{0}\" type=\"video/mp4\"/>\n","             </video>'''.format(encoded.decode('ascii')))"],"execution_count":3,"outputs":[{"output_type":"display_data","data":{"text/plain":"Output hidden; open in https://colab.research.google.com to view."},"metadata":{}}]},{"cell_type":"code","metadata":{"id":"71joDP5DXI_p","executionInfo":{"status":"ok","timestamp":1634572040793,"user_tz":-540,"elapsed":347,"user":{"displayName":"송정현","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"03834848729540117659"}}},"source":["# dnn module의 위치 정의\n","model_name = '/content/gdrive/MyDrive/DNN_Face_Detection/res10_300x300_ssd_iter_140000.caffemodel'   # caffemodel의 weight 값\n","prototxt_name = '/content/gdrive/MyDrive/DNN_Face_Detection/deploy.prototxt.txt'                     # model Architecture 에 대한 정보                \n","min_confidence = 0.5  # detection 으로 인정할 최소 확률(신뢰도)\n","file_name = '/content/gdrive/MyDrive/DNN_Face_Detection/video/kim.mp4' # Detection 할 원본 동영상\n","output_name = 'output_video.mp4'    # detection 된 output 동영상"],"execution_count":4,"outputs":[]},{"cell_type":"code","metadata":{"id":"z5HzSPwKXJCd","executionInfo":{"status":"ok","timestamp":1634572044734,"user_tz":-540,"elapsed":319,"user":{"displayName":"송정현","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"03834848729540117659"}}},"source":["def detectAndDisplay(frame):\n","    # caffemodel의 weight 값과 모델 네트워크 구성을 불러와서 모델을 정의\n","    model = cv2.dnn.readNetFromCaffe(prototxt_name, model_name)\n","\n","    # 이미지를 300x300 으로 size를 조정하고 blob 를 만든다.\n","    blob = cv2.dnn.blobFromImage(cv2.resize(frame, (300, 300)), 1.0,\n","            (300, 300), (104.0, 177.0, 123.0))\n","\n","    model.setInput(blob)\n","    detections = model.forward()\n","    \n","    # detections 한 수만큼 루프가 돈다.\n","    for i in range(0, detections.shape[2]):\n","            confidence = detections[0, 0, i, 2]  # confidence 는 detection한 확률을 나타냄\n","\n","            # min_confidence 보다 큰 경우에만 detection 으로 인정함\n","            if confidence > min_confidence:\n","                    (height, width) = frame.shape[:2]\n","                    # detection 된 영역을 boxing\n","                    # 상대적 좌표 * np.array([width, height, width, height]) 절대적인 boxing 좌표을 구해낸다. \n","                    box = detections[0, 0, i, 3:7] * np.array([width, height, width, height])\n","                    (startX, startY, endX, endY) = box.astype(\"int\")\n","     \n","                    # 얼굴에 bounding box(사각형)를 그리고 확률값도 함께 나타낸다\n","                    text = \"{:.2f}%\".format(confidence * 100)\n","                    y = startY - 10 if startY - 10 > 10 else startY + 10\n","                    cv2.rectangle(frame, (startX, startY), (endX, endY),\n","                            (0, 255, 0), 2)\n","                    cv2.putText(frame, text, (startX, y),\n","                            cv2.FONT_HERSHEY_SIMPLEX, 0.5, (0, 255, 0), 1)\n","\n","    # video 를 disk 에 output 하기 위해 writer 를 초기화한다.\n","    global writer\n","    if writer is None and output_name is not None:\n","        fourcc = cv2.VideoWriter_fourcc(*\"DIVX\")\n","        writer = cv2.VideoWriter(output_name, fourcc, 20,\n","                (frame.shape[1], frame.shape[0]), True)\n","        \n","    # disk 에 frame 을 write 합니다.\n","    if writer is not None:\n","        writer.write(frame)"],"execution_count":5,"outputs":[]},{"cell_type":"code","metadata":{"id":"_VDr7lFWXJE2","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1634572130503,"user_tz":-540,"elapsed":23776,"user":{"displayName":"송정현","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"03834848729540117659"}},"outputId":"808163fe-ce35-4655-f9d8-46d1d742f334"},"source":["# 원본 동영상에서 video stream을 읽어온다.\n","cap = cv2.VideoCapture(file_name)\n","writer = None\n","if not cap.isOpened:\n","    print('--(!)Error opening video capture')\n","    exit(0)\n","while True:\n","    ret, frame = cap.read()\n","    if frame is None:\n","        # close the video file pointers\n","        cap.release()\n","        # close the writer point\n","        writer.release()\n","        print('--(!) No captured frame -- Break!')\n","        break\n","    detectAndDisplay(frame)"],"execution_count":7,"outputs":[{"output_type":"stream","name":"stdout","text":["--(!) No captured frame -- Break!\n"]}]}]}